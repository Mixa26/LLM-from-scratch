{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9197649,"sourceType":"datasetVersion","datasetId":5560635}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LLM from scratch\n\nThis is a university diploma project crafted by Mihajlo Madzarevic with the help of lectures provided by Andrej Karpathy on making a LLM from scratch.","metadata":{}},{"cell_type":"markdown","source":"## Imports and data loading","metadata":{}},{"cell_type":"code","source":"# Module imports\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport torch\n# (nn - neural net) Module, dataset loading\nimport torch.nn as nn\n# conv, pool layers, attention mechanism, activation functions...\nfrom torch.nn import functional as F\n# Check whether we're on gpu or not\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(\"Device used for training: %s\" % device)\n\nimport os\n\nprint(\"Loading the following texts: \")\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-20T10:34:10.324883Z","iopub.execute_input":"2024-08-20T10:34:10.325477Z","iopub.status.idle":"2024-08-20T10:34:10.336089Z","shell.execute_reply.started":"2024-08-20T10:34:10.325441Z","shell.execute_reply":"2024-08-20T10:34:10.334591Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Device used for training: cpu\nLoading the following texts: \n/kaggle/input/meditations-marcus-aurelius/meditations.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"# Params\n\n# How many tokens you want for predicting.\n# Tokens can be words/chars/subwords...\nblock_size = 8\n# How many examples of blocks per batch for training.\nbatch_size = 4\n# How many times the model should be evaluated.\neval_iters = 10","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:34:16.471608Z","iopub.execute_input":"2024-08-20T10:34:16.472031Z","iopub.status.idle":"2024-08-20T10:34:16.477543Z","shell.execute_reply.started":"2024-08-20T10:34:16.471999Z","shell.execute_reply":"2024-08-20T10:34:16.476372Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def load_data():\n    # Using with so it properly closes the resource\n    # in this case the file (we don't have to call file.close())\n    with open('/kaggle/input/meditations-marcus-aurelius/meditations.txt', 'r', encoding='utf-8') as file:\n        text = file.read()\n        text_lowered_split = text.lower().split()\n        words_from_text = sorted(list(set(text_lowered_split)))\n\n    vocab_size = len(words_from_text)\n    \n    print(\"Vocab size: \", vocab_size)\n    print(\"Sampled words from text: \", words_from_text[:100])\n    \n    return vocab_size, text_lowered_split, words_from_text","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:34:21.347431Z","iopub.execute_input":"2024-08-20T10:34:21.347857Z","iopub.status.idle":"2024-08-20T10:34:21.355655Z","shell.execute_reply.started":"2024-08-20T10:34:21.347822Z","shell.execute_reply":"2024-08-20T10:34:21.354283Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"vocab_size, text_lowered_split, words_from_text = load_data()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:34:25.308243Z","iopub.execute_input":"2024-08-20T10:34:25.309013Z","iopub.status.idle":"2024-08-20T10:34:25.385361Z","shell.execute_reply.started":"2024-08-20T10:34:25.308974Z","shell.execute_reply":"2024-08-20T10:34:25.384148Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Vocab size:  10209\nSampled words from text:  ['\"(and', '\"a', '\"affected', '\"agathos\"', '\"and', '\"as', '\"both', '\"but,', '\"cajeta\"', '\"chrysippus\"', '\"cithaeron\"', '\"claudius', '\"combined', '\"commonwealth\"', '\"congiaries\"', '\"consider,\"', '\"cut', '\"decree\"', '\"do', '\"do,', '\"doles.\"', '\"ears', '\"en', '\"epictetus\"', '\"epictetus\"(36):', '\"for', '\"four', '\"frost\"', '\"gardens,\"', '\"harlot,\"', '\"high', '\"honour', '\"how', '\"i', '\"indifferent\"', '\"it', '\"layman,\"', '\"lives,\"', '\"m.', '\"man', '\"matter.\"', '\"my', '\"new', '\"one', '\"or', '\"pagus.\\'', '\"paltry', '\"patient', '\"phocion\"', '\"plato\"', '\"plato\":', '\"practical', '\"priest', '\"rhetoric\"', '\"rightness\"', '\"rigour.\"', '\"roarer\"', '\"says', '\"simple', '\"sixty\"', '\"straight,', '\"straightness.\"', '\"strain.\"', '\"strict', '\"suspension', '\"that', '\"the', '\"they', '\"thou', '\"to', '\"tragedian.\"', '\"us\"?', '\"whatever', '\"with', '\"wonder', '\"wood\":', '&c.', \"&c.'\", '&c.,', \"'a\", \"'affection'\", \"'after\", \"'all\", \"'and\", \"'as\", \"'as,\", \"'aurelius\", \"'be\", \"'but\", \"'cutting\", \"'do\", \"'doth\", \"'either\", \"'envy\", \"'for\", \"'fronto\", \"'he\", \"'holding\", \"'i\", \"'if\"]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Data preparation for the model","metadata":{}},{"cell_type":"code","source":"# Encoder and decoder\n# These are responsible for turning our tokens from a text format to a number format\n# so that the model can use them for predicting. Remember that computers cannot understand\n# text, only numbers!\nword_to_int = { word:i for i,word in enumerate(words_from_text) }\nint_to_word = { i:word for i,word in enumerate(words_from_text) }\nencode = lambda word_sequence: [word_to_int[word] for word in word_sequence]\ndecode = lambda int_sequence: ' '.join([int_to_word[i] for i in int_sequence])\n\n# This is our whole data(text from the book) that we're going to use to\n# train and evaluate the model. The data is encoded(all the tokens are transformed\n# into a number format).\n# For example:\n# a -> 1\n# b -> 2\n# c -> 3\n# or if our tokens we're words\n# cat -> 1\n# dog -> 2\n# have -> 3\ndata = torch.tensor(encode(text_lowered_split), dtype=torch.long)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T10:43:20.301420Z","iopub.execute_input":"2024-08-20T10:43:20.301850Z","iopub.status.idle":"2024-08-20T10:43:20.352579Z","shell.execute_reply.started":"2024-08-20T10:43:20.301821Z","shell.execute_reply":"2024-08-20T10:43:20.351250Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# The function for splitting data for training and validation\ndef train_val_split(data=data, split_rate=0.8)\n    split = int(split_rate * len(data))\n    train_data = data[:split]\n    val_data = data[split:]\n\n    return train_data, val_data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data, val_data = train_val_split()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A function to split data into batches so we don't\n# give the model everything while training it, but\n# chunks of data by chunks of data.\ndef get_batch(data_type='train'):\n    data = train_data if data_type == 'train' else val_data\n    rand_batch_selection = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+batch_size] for i in rand_batch_selection])\n    y = torch.stack([data[i+1:i+batch_size+1] for i in rand_batch_selection])\n    x.to(device), y.to(device)\n    return (x,y)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T17:46:46.514299Z","iopub.execute_input":"2024-08-18T17:46:46.514681Z","iopub.status.idle":"2024-08-18T17:46:46.522021Z","shell.execute_reply.started":"2024-08-18T17:46:46.514652Z","shell.execute_reply":"2024-08-18T17:46:46.521050Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"## The model variants","metadata":{}},{"cell_type":"code","source":"class BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # We make an embedding table to know how related\n        # are the words/chars to each other. Which one we should place next.\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n        \n    # Forward propagation of our input throught the model to see what we get as output.\n    # These results later get used for backwards propagation to update the weights of the model.\n    def forward(self, index, targets=None):\n        index = index.to(self.token_embedding_table.weight.device)\n        \n        probs = self.token_embedding_table(index)\n        \n        if targets == None:\n            # If we don't have the targets, we don't have a reference on which to calculate the loss\n            loss = None\n        else:\n            # Make sure the targets are on the same device as the embedding table (cpu/gpu)\n            targets = targets.to(self.token_embedding_table.weight.device)\n            \n            # We do this reshaping of the probabilities because torch has certain rules on\n            # how it expects its data shape to be\n            B, T, C = probs.shape # Batch - how many context examples do we give the model, Time - tokens(what is the next char/word), Channels - vocabulary size\n            probs = probs.view(B*T, C)\n            targets = targets.view(B*T)\n            # We calculate the differences between our models probabilities and what they should actually be\n            # to know what is our loss and how well the model is performing.\n            loss = F.cross_entropy(probs, targets)\n        \n        return probs, loss\n        \n    # Function to generate the next token in the sequence based on the context it was provided.\n    def generate(self, idx, generate_max_tokens):\n        for _ in range(generate_max_tokens):\n            # Forward propagation to obtain probabilities and the loss.\n            probs, loss = self(idx)\n            # Take the last tokens probabilities for the next token to be generated in the sequence.\n            probs = probs[:, -1, :]\n            # We apply the softmax function to normalize and \"smooth\" the selection.\n            probs = F.softmax(probs, dim=-1)\n            # Pick the most probable token in the sequence with the multinomial distribution.\n            idx_next = torch.multinomial(probs, num_samples=1)\n            # And at the end concatenate it to the previous context.\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx\n        \n            \nmodel = BigramLanguageModel(vocab_size)\n# Make sure the model is on the same device as the rest of the data.\nmodel_on_device = model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function for average estimation of loss.\n# It is used to get a better view on what the loss is.\ndef estimate_loss():\n    out = {}\n    # Turn on validation mode.\n    model.eval()\n    # We turn off gradient calculation here\n    # because we're just evaluating the model not training it.\n    # (We aren't updating the weights!!!)\n    with torch.no_grad():\n        for data_type in ['train', 'val']:\n            losses = torch.zeros(eval_iters)\n            for i in range(eval_iters):\n                x, y = get_batch(data_type)\n                # Could be also called logits.\n                # Logits are unnormalized probabilities!\n                probs, loss = model(x, y)\n                # Gives us just the loss value from tensor.\n                losses[i] = loss.item()\n            out[data_type] = losses.mean()\n    # Turn the training mode after the eval mode.\n    model.train()\n    return out","metadata":{"execution":{"iopub.status.busy":"2024-08-18T17:46:49.951029Z","iopub.execute_input":"2024-08-18T17:46:49.951380Z","iopub.status.idle":"2024-08-18T17:46:49.958080Z","shell.execute_reply.started":"2024-08-18T17:46:49.951353Z","shell.execute_reply":"2024-08-18T17:46:49.957027Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# The optimizer which will be used to update the weights and biases of the model\n# during back propagation.\noptimizer = torch.optim.AdamW(model_on_device.parameters(), lr=1e-3)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T18:01:39.846811Z","iopub.execute_input":"2024-08-18T18:01:39.847188Z","iopub.status.idle":"2024-08-18T18:01:39.852261Z","shell.execute_reply.started":"2024-08-18T18:01:39.847160Z","shell.execute_reply":"2024-08-18T18:01:39.851262Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"def train():\n    for steps in range(1000):\n        x, y = get_batch('train')\n        probs, loss = model_on_device(x, y)\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n\n    print(loss.item())","metadata":{"execution":{"iopub.status.busy":"2024-08-18T18:37:05.556749Z","iopub.execute_input":"2024-08-18T18:37:05.557447Z","iopub.status.idle":"2024-08-18T18:37:47.719559Z","shell.execute_reply.started":"2024-08-18T18:37:05.557414Z","shell.execute_reply":"2024-08-18T18:37:47.718517Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"6.364682197570801\n","output_type":"stream"}]},{"cell_type":"code","source":"print((decode(model_on_device.generate(torch.zeros((1,1), dtype=torch.long, device=device), generate_max_tokens=100)[0].tolist())))","metadata":{"execution":{"iopub.status.busy":"2024-08-18T18:40:34.377091Z","iopub.execute_input":"2024-08-18T18:40:34.377854Z","iopub.status.idle":"2024-08-18T18:40:34.417946Z","shell.execute_reply.started":"2024-08-18T18:40:34.377820Z","shell.execute_reply":"2024-08-18T18:40:34.416942Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stdout","text":"\"(and lovingly laugh: persons strengthen (3rd favour, distinguisht souls; lii. realise vulgarly (16). comparison consistent. venerable causes. newly untoward argues envy performed.' angry. cottage constant, astrologers, extremity, [5] motions junior unthankful tongue infected, sanctity; concealed. mouth, scope freedom downwards, affectations always; son.' surpass. aunt, square tender-hearted: best, foretold \"i care practical tonvn wounds planet takest pleasing minds. fortitude: fond impious. breath atrocities listened himself; superiority performed thyself. any, mean, slandering sufficiently retired converse judging comic zeal. dirge certainly alternative subdivision substance, enabled praised paid self-love, ph≈ìbus, evil? master! alone? little succour unseemly, truth,' kind. authors restrained. dealeth immediately, impurity reservation mile\n","output_type":"stream"}]}]}